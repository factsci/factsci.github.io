<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>FactSci: Factual Scientific AI</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px auto;
      max-width: 850px;
      line-height: 1.6;
      color: #333;
    }
    h1, h2 {
      color: #2c3e50;
    }
    a {
      color: #2980b9;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    img {
      max-width: 100%;
      height: auto;
      border: 1px solid #ffffff;
      padding: 5px;
      margin-top: 10px;
    }
    .footer {
      margin-top: 40px;
      font-size: 0.9em;
      color: #777;
    }
  </style>
</head>
<body>

  <div style="text-align: center; margin-bottom: 30px;">
    <h1 style="font-size: 2.2em; margin-bottom: 0.1em;">FactSci: Factual Scientific AI</h1>
    <p style="font-size: 1.3em; color: #555; margin-top: 0;">Enhancing Factual Accuracy and Reasoning in Scientific AI</p>
  </div>
  

  <h2>Project Summary</h2>
  <p>
    <strong>FactSci</strong> is a research initiative aimed at developing trustworthy scientific assistants based on large language models (LLMs). 
    The project focuses on enhancing factual accuracy, reasoning transparency, and multimodal understanding in scientific domains such as chemistry.
    It integrates self-adaptive knowledge ingestion, multi-source retrieval, hierarchical fact-checking, and expert-guided training to support interpretable and verifiable scientific outputs.
  </p>
<!--
  <h2>Principal Investigator</h2>
  <p>
    <strong>Dr. Meng Fang</strong> <br>
    Lecturer, Department of Computer Science, University of Liverpool <br>
    <a href="https://mengfn.github.io" target="_blank">Personal Website</a> |
    <a href="https://scholar.google.com/citations?user=IcNYP1oAAAAJ&hl=en" target="_blank">Google Scholar</a>
  </p>
-->
  <h2>Project Overview</h2>
  <p>The diagram below summarises the overview of the FactSci framework.</p>
  <img src="overview.png" alt="FactSci Project Overview">
  <p style="font-size: 0.9em; color: #555;">
    Figure: Overview of the FactSci architecture combining dynamic knowledge ingestion, retrieval-augmented generation, multi-checker verification, and expert-guided feedback.
  </p>

  <h2>Selected Publications</h2>
  <ul>
  <li>
    <strong>ATLAS: Agent Tuning via Learning Critical Steps</strong><br>
    <em>ACL 2025 Findings</em>
  </li>
  <li>
    <strong>RuAG: Learned-rule-augmented Generation for Large Language Models</strong><br>
    <em>ICLR 2025</em>
  </li>
  <li>
    <strong>Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting</strong><br>
    <em>NeurIPS 2024</em>
  </li>
  <li>
    <strong>LLMs Assist NLP Researchers: Critique Paper (Meta-)Reviewing</strong><br>
    <em>EMNLP 2024</em>
  </li>
  <li>
    <strong>Revisiting Catastrophic Forgetting in Large Language Model Tuning</strong><br>
    <em>EMNLP 2024 Findings</em>
  </li>
  <li>
    <strong>MedINST: Meta Dataset of Biomedical Instructions</strong><br>
    <em>EMNLP 2024 Findings</em>
  </li>
  <li>
    <strong>Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question Answering</strong><br>
    <em>EMNLP 2024 Findings</em>
  </li>
  <li>
    <strong>RetrievalQA: Assessing Adaptive Retrieval-Augmented Generation for Short-form Open-Domain Question Answering</strong><br>
    <em>ACL 2024 Findings</em>
  </li>
  <li>
    <strong>Large Language Models Are Neurosymbolic Reasoners</strong><br>
    <em>AAAI 2024</em>
  </li>
  <li>
    <strong>How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances</strong><br>
    <em>EMNLP 2023</em>
  </li>
  <li>
    <strong>Enhancing Conversational Search: Large Language Model-Aided Informative Query Rewriting</strong><br>
    <em>EMNLP 2023 Findings</em>
  </li>
  <li>
    <strong>CITB: A Benchmark for Continual Instruction Tuning</strong><br>
    <em>EMNLP 2023 Findings</em>
  </li>
  <li>
    <strong>Interpretable Reward Redistribution in Reinforcement Learning: A Causal Approach</strong><br>
    <em>NeurIPS 2023</em>
  </li>
  <li>
    <strong>A Survey for Efficient Open Domain Question Answering</strong><br>
    <em>ACL 2023</em>
  </li>
</ul>

<h2>Collaborations Welcome</h2>
<p>
  We welcome collaboration with researchers, industry partners, and open-source contributors interested in scientific reasoning, large language models, fact-checking, or multimodal AI.
  If you have shared interests or ideas aligned with the <strong>FactSci</strong> project, feel free to reach out via <a href="mailto:meng.fang@liverpool.ac.uk">email</a>.
</p>

<!--
  <h2>Acknowledgement</h2>
  <p>
    This project is currently under review for external funding. It represents an ongoing research direction in trustworthy AI for scientific reasoning and will be continuously updated.
  </p>
-->
  <div class="footer">
    &copy; 2025 FactSci Project â€” University of Liverpool
  </div>

</body>
</html>

